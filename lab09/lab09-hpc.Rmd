---
title: "Lab 9 - HPC"
output: 
  # pdf_document: default
  # html_document: default
  tufte::tufte_html:
    css: style.css
link-citations: yes
---

# Learning goals

In this lab, you are expected to learn/put in practice the following skills:

- Evaluate whether a problem can be parallelized or not.
- Practice with the parallel package.
- Use Rscript to submit jobs.

```{r eval=FALSE, echo=FALSE}
# install any missing packages
install.packages("microbenchmark")
```

```{r echo=FALSE, message=FALSE}
library(tidyverse)
library(parallel)
```

## Problem 1: Think

Give yourself a few minutes to think about what you learned about parallelization. List three examples of problems that you believe may be solved using parallel computing, and check for packages on the HPC CRAN task view that may be related to it.

_Answer here._ 

1. Random number/sample generation: If we need to randomly generate a large amount of data, instead of using a `for` loop and generate one number at a time, we can use parallel computing where multiple processors can generate data simultaneously. Some R packages related to this task are `rlecuyer`, `rstream`, `sitmo`, and `dqrng`.

2. Batch computation: This is mainly used in training deep learning models. Instead of training one data sample at a time, we can train a batch of data in parallel using a GPU. Some batch managing packages include `batch`, `BatchJobs`, and `batchtools`. Some packages that invoke GPUs are `rgpu`, `gcbd`, and `GPUmatrix`. An example package of deep learning is `tensorflow`.

3. Large memory computation: When the computation we need to do is in large scale, we can use parallelization to reduce the amount of data needed to be stored in memory. Some R packages of conducting this task are `biglm`, `ff`, and `bigmemory`.

## Problem 2: Pre-parallelization

The following functions can be written to be more efficient without using
`parallel`:

1. This function generates a `n x k` dataset with all its entries having a Poisson distribution with mean `lambda`.

```{r p2-fun1}
fun1 <- function(n = 100, k = 4, lambda = 4) {
  x <- NULL

  for (i in 1:n) {
    x <- rbind(x, rpois(k, lambda))
  }
  return(x)
}

fun1alt <- function(n = 100, k = 4, lambda = 4) {
  x <- matrix(rpois(n * k, lambda), nrow = n, ncol = k)
  return(x)
}

# Benchmarking
microbenchmark::microbenchmark(
  fun1(),
  fun1alt()
)
```

How much faster?

_Answer here._ The `fun1alt` function is approximately 12 times faster than the `fun1` function.

2.  Find the column max (hint: Checkout the function `max.col()`).

```{r p2-fun2}
# Data Generating Process (10 x 10,000 matrix)
set.seed(1234)
x <- matrix(rnorm(1e4), nrow = 10)

# Find each column's max value
fun2 <- function(x) {
  apply(x, 2, max)
}

fun2alt <- function(x) {
  x[cbind(max.col(t(x)), 1:ncol(x))]
}

# Benchmarking
mbm <- microbenchmark::microbenchmark(
  fun2(x),
  fun2alt(x)
)
mbm
```

_Answer here with a plot._

```{r}
mbm |>
  ggplot(aes(x = time, y = expr, fill = expr, color = expr)) +
  geom_boxplot()
```

## Problem 3: Parallelize everything

We will now turn our attention to non-parametric 
[bootstrapping](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)).
Among its many uses, non-parametric bootstrapping allow us to obtain confidence intervals for parameter estimates without relying on parametric assumptions.

The main assumption is that we can approximate many experiments by resampling observations from our original dataset, which reflects the population. 

This function implements the non-parametric bootstrap:

```{r p3-boot-fun}
my_boot <- function(dat, stat, R, ncpus = 1L) {
  # Getting the random indices
  n <- nrow(dat)
  idx <- matrix(sample.int(n, n * R, TRUE), nrow = n, ncol = R)

  # Making the cluster using `ncpus`
  # STEP 1: GOES HERE
  cl <- makePSOCKcluster(ncpus)
  # STEP 2: GOES HERE
  clusterExport(cl, varlist = c("idx", "dat", "stat"), envir = environment())
  # STEP 3: THIS FUNCTION NEEDS TO BE REPLACED WITH parLapply
  # ans <- lapply(seq_len(R), function(i) {
  #   stat(dat[idx[,i], , drop=FALSE])
  # })

  ans <- parLapply(cl, seq_len(R), function(i) {
    stat(dat[idx[,i], , drop=FALSE])
  })

  # Coercing the list into a matrix
  ans <- do.call(rbind, ans)

  # STEP 4: GOES HERE
  stopCluster(cl)
  ans
}
```

1. Use the previous code, and make it work with `parallel`. Here is just an example
for you to try:

```{r p3-test-boot}
# Bootstrap of a linear regression model
my_stat <- function(d) {
  coef(lm(y ~ x, data = d))
}

# DATA SIM
set.seed(1)
n <- 500
R <- 1e4
x <- cbind(rnorm(n))
y <- x * 5 + rnorm(n)

# Check if we get something similar as lm
ans0 <- confint(lm(y ~ x))
ans1 <- my_boot(dat = data.frame(x, y), my_stat, R = R, ncpus = 8)

ans0
t(apply(ans1, 2, quantile, probs = c(0.025, 0.975)))
```

2. Check whether your version actually goes faster than the non-parallel version:

```{r benchmark-problem3}
# your code here
detectCores()

system.time(my_boot(dat = data.frame(x, y), my_stat, R = R, ncpus = 1))

system.time(my_boot(dat = data.frame(x, y), my_stat, R = R, ncpus = 8))
```

_Answer here._ The parallel version does take less time to run than the non-parallel version.

## Problem 4: Compile this markdown document using Rscript

Once you have saved this Rmd file, try running the following command
in your terminal:

```bash
Rscript --vanilla -e 'rmarkdown::render("lab09-hpc.Rmd")' &
```

Where `[full-path-to-your-Rmd-file.Rmd]` should be replace with the full path to
your Rmd file... :).


